\chapter{Unsupervised learning}

\section{Setting}
Supervised learning requires the availability of labelled examples.
Labelling examples can be an extremely expensive process and sometimes the labels are not known.
Unsupervised techniques can be employed to group examples into clusters.

\section{K-means clustering}

	\subsection{Setting}
	K-means clustering assumes that examples should be grouped into $k$ cluster, with each cluster $i$ represented by its measn $\vec{\mu}_i$.

	\subsection{Algorithm}
	
	\begin{enumerate}
		\item Initialize cluster means $\vec{\mu}_1,\dots,\vec{\mu}_k$.
		\item Iterate until no mean changes:
			\begin{enumerate}
				\item Assign each example to the cluster with the nearest mean.
				\item Update cluster means according to assigned examples.
			\end{enumerate}
	\end{enumerate}

	\subsection{Defining similarity}

		\subsubsection{Similarity measures}

			\paragraph{Standard euclidean distance}
			The standard euclidean distance in $\mathbb{R}^D$ is:

			$$d(\vec{x},\vec{x}') = \sqrt{\sum\limits_{i=1}^d(x_i-x_i')^2}$$

			\paragraph{Generic Minkowski metric}
			The generic Minkowski metric for $p\ge 1$ is:

			$$d(\vec{x},\vec{x}') = \biggl(\sum\limits_{i=1}^d|x_i-x_i'\biggr)^\frac{1}{p}$$

			\paragraph{Cosine similarity}
			The cosine similarity is the cosine of the angle between vector and is:

			$$s(\vec{x},\vec{x}') = \frac{\vec{x}^T\vec{x}'}{||\vec{x}||||\vec{x};||}$$

		\subsubsection{Define the quality of the obtained clusters}
		The sum-of-squared error criterion is used to define the quality of the obtained clusters.
		Let $n_i$ be the number of samples in cluster $\mathcal{D}_i$ and $\vec{\mu}_i$ the cluster sample mean:

		$$\vec{\mu}_i = \frac{1}{n_i}\sum\limits_{\vec{x}\in\mathcal{D}_i}\vec{x}$$

		The sum-of-squared errors is defined as:

		$$E = \sum\limits_{i=1}^k\sum\limits_{\vec{x}\in\mathcal{D}_i}||\vec{x}-\vec{\mu}_i||^2$$

\section{Gaussian Mixture model GMM}
In the Gaussian mixture model GMM the examples are clustered using a mixture of Gaussian distributions.
Assuming that a number of Gaussians is given it estimates mean and variance of each Gaussian.

	\subsection{Parameter estimation}
	Maximum likelihood estimation cannot be applied as cluster assignment is unknown.
	So the expectation-maximisation approach is used:

	\begin{enumerate}
		\item Compute expected cluster assignment given current parameter setting.
		\item Estimate parameters given cluster assignment.
		\item Iterate
	\end{enumerate}

	\subsection{Estimating means of $k$ univariate Gaussians}

		\subsubsection{Setting}
		A dataset of $x_1,\dots,x_n$ examples is observed.
		For each example $x_i$ the cluster assignment is modelled as $z_{i1}, \dots, z_{ik}$ binary latent variables.
		$z_{ij}=1$ if Gaussian $j$ generated $x_{i}$ and $0$ otherwise.
		Parameters to be estimated are $\mu_1, \dots,\mu_k$ the Gaussians means.
		All Gaussians are assumed to have the same known variance $\sigma^2$.

		\subsubsection{Algorithm}
		
		\begin{enumerate}
			\item Initialize $h = \langle\mu_1,\dots,\mu_k\rangle$
			\item Iterate until difference in maximum likelihood is below a certain threshold:

				\begin{itemize}
					\item[E-step] Calculate expected value $\mathbb{E}[z_{ij}]$ for each latent variable assuming current hypothesis $h = \langle \mu_1, \dots,\mu_k\rangle$ holds.
					\item[M-step] Calculate a new maximum likelihood hypotheses $h' = \langle\mu_1',\dots,\mu_k'\rangle$ assuming values of latent variables are their expected values just composed.
						Replace $h\leftarrow h'$.
				\end{itemize}
		\end{enumerate}

			\paragraph{E-step}
			The expected value of $z_{ij}$ is the probability that $x_i$ is generated by the Gaussian $j$ assuming hypothesis $h = \langle\mu_1,\dots,\mu_k\rangle$ holds:

			$$\mathbb{E}[z_{ij}] = frac{p(x_i|\mu_j)}{\sum\limits_{l=1}^kp(x_i|\mu_l)} = \frac{e^{-\frac{1}{2\sigma^2}(x_i-\mu_j)^2}}{\sum\limits_{l=1}^k e^{-\frac{1}{2\sigma^2}(x_i-\mu_l)^2}}$$

			\paragraph{M-step}
			The maximum-likelihood mean $\mu_j$ is the weighted sample mean, each instance being weighted by its probability of being generated by Gaussian $j$:

			$$\mu_j' = \frac{\sum\limits_{i=1}^n\mathbb{E}[z_{ij}]x_i}{\sum\limits_{i=1}^n\mathbb{E}[z_{ij}]}$$

\section{Expectation-Maximization (EM)}

	\subsection{Formal setting}
	Given a dataset made of an observed part $X$ and an unobserved part $Z$, there is a need to estimate the hypothesis maximizing the expected log-likelihood for the data, with expectation taken over unobserved data:

	$$h^* = \arg\max\limits_{h}\mathbb{E}_z[\ln p(X,Z|h)]$$

	The  unobserved data $Z$ should be treated as random variables governed by the distribution depending on $X$ and $h$.
	
	\subsection{Generic algorithm}

	\begin{enumerate}
		\item Initialize hypothesis $h$.
		\item Iterate until convergence:
			
			\begin{itemize}
				\item[E-step] Compute the expected likelihood of an hypothesis $h'$ for the full data, where the unobserved data distribution is modelled according to the current hypothesis $h$ and the observed data:

					$$Q(h;h') = \mathbb{E}_z[\ln p(X,Z|h')|h,X]$$
					
				\item[M-step] Replace the current hypothesis with the one maximizing $Q(h';h)$:

					$$h\leftarrow\arg\max\limits_{h'}Q(h';h)$$
			\end{itemize}
	\end{enumerate}

	\subsection{Estimating means of $k$ univariate Gaussians}

		\subsubsection{Derivation}
		The likelihood of an example is:

		$$p(x_i, z_{i1},\dots,z_{ij}|h') = \frac{1}{\sqrt{2\pi}\sigma}e^{-\sum\limits_{j=1}^k z_{ij}\frac{(x_i-\mu_j)^2}{2\sigma^2}}$$

		The dataset log-likelihood is:

		$$\ln p(X,Z|h) = \sum\limits_{i=1}^n\biggl(\ln\frac{1}{\sqrt{2\pi}\sigma}-\sum\limits_{j=1}^kz_{ij}\frac{x_i-\mu_j')}{2\sigma^2}\biggr)$$

		\subsubsection{E-step}
		The expected log-likelihood, remembering the linearity of the expectation operator:

		\begin{align*}
			\mathbb{E}_z[\ln p(X,Z|h')] &=\mathbb{E}_z\biggl[\sum\limits_{i=1}^n\biggl(\ln\frac{1}{\sqrt{2\pi}\sigma}-\sum\limits_{j=1}^kz_{ij}\frac{(x_i-\mu_j)^2}{2\sigma^2}\biggr)\biggr]=\\
						    &=\sum\limits_{i=1}^n\biggl(\ln\frac{1}{\sqrt{2\pi}\sigma}-\sum\limits_{j=1}^k\mathbb{E}[z_{ij}]\frac{(x_i-\mu_j')^2}{2\sigma^2}\biggr)
		\end{align*}

		The expectation given current hypothesis $h$ and observed data $X$ is computed as:

		$$\mathbb{E}[z_{ij}] = \frac{p(x_i|\mu_j)}{\sum\limits_{l=1}^kp(x_i|\mu_l)} = \frac{e^{-\frac{1}{2\sigma^2}(x_i-\mu_j)^2}}{\sum\limits_{l=1}^ke^{-\frac{1}{2\sigma^2}(x_i-\mu_l)^2}}$$

		\subsubsection{M-step}
		The likelihood maximization gives:

		\begin{align*}
			\arg\max\limits_{h'}Q(h',h) &= \arg\max\limits_{h'}\sum\limits_{i=1}^n\biggl(\ln\frac{1}{\sqrt{2\pi}\sigma}-\sum\limits_{j=1}^k\mathbb{E}[z_{ij}]\frac{(x_i-\mu_j')^2}{2\sigma^2}\biggr)=\\
						    &=\arg\min\limits_{h'}\sum\limits_{i=1}^n\sum\limits_{j=1}^k\mathbb{E}[z_{ij}](x_i\mu_j')^2
		\end{align*}

		Zeroing the derivative with respect to each mean:

		\begin{align*}
			\frac{\partial}{\partial\mu_j} &= -2\sum\limits_{i=1}^n\mathbb{E}[z_{ij}](x_i-\mu_j') = 0\\
			\mu_j' &=\frac{\sum\limits_{i=1}^n\mathbb{E}[z_{ij}]x_i}{\sum\limits_{i=1}^n\mathbb{E}[z_{ij}]}
		\end{align*}





\section{Choosing the number of clusters}

	\subsection{Elbow method}

		\subsubsection{Idea}
	Increasing the number of clusters allows for better modelling of data.
	There is a need to trade-off the quality of the cluster with their quantity so the number of clusters stop being increased when the advantage is limited.
	This method can be ambiguous with multiple candidate points.

		\subsubsection{Approach}

		\begin{enumerate}
			\item Run clustering algorithm for increasing number of clusters.
			\item Plot clustering evaluation metric like the sum of squared errors for different $k$.
			\item Choose $k$ when there is an angle making an angle in the plot, or a drop in gain.
		\end{enumerate}


	\subsection{Average silhouette method}

		\subsubsection{Idea}
		Increasing the number of clusters makes each cluster more homogeneous and make different clusters more similar.
		A quality metric that trades-off intra-cluster similarity and inter-cluster dissimilarity can be used.

		\subsubsection{Silhouette coefficient for example $i$}

		\begin{enumerate}
			\item Compute the average dissimilarity between $i$ and examples of its cluster $C$:

				$$a_i = d(i,C) = \frac{1}{|C|}\sum\limits_{j\in C}d(i,j)$$

			\item Compute the average dissimilarity between $i$ and examples of each clusters $C'\neq C$ and take the minimum:

				$$b_i = \min\limits_{C'\neq C}d(i,C')$$

			\item The silhouette coefficient is:

				$$s_i = \frac{b_i-a_i}{\max(a_i, b_i)}$$

		\end{enumerate}

		\subsubsection{Approach}

		\begin{enumerate}
			\item Run a cluster algorithm for increasing number of clusters.
			\item Plot the average over the examples silhouette coefficient for different $k$.
			\item Choose $k$ where the average silhouette coefficient is maximal.
		\end{enumerate}

\section{Hierarchical clustering}

	\subsection{Setting}
	Clustering does not need to be flat: natural grouping of data is often hierarchical and a hierarchy of clusters can be built on examples.

	\subsection{Top-down approach}

	\begin{enumerate}
		\item Start from a single cluster with all examples.
		\item Recursively split clusters into subclusters.
	\end{enumerate}

	\subsection{Bottom-up approach}

	\begin{enumerate}
		\item Start with $n$ clusters of individual examples or singletons.
		\item Recursively aggregate pairs of clusters.
	\end{enumerate}

	\subsection{Dendograms}
	A dendogram is a representation of hierarchical clusters.
	It is a tree where the root is the total dataset and each child represent a subcluster.
	The leaves are the examples.

	\subsection{Agglomeritive hierarchical clustering}

		\subsubsection{Algorithm}

		\begin{enumerate}
			\item Initialize: final cluster number $k$, initial cluster number $\hat{k} = n$ and initial clusters $\mathcal{D}_i = \{x_i\},i\in\{1,\dots,n\}$.
			\item While $\hat{k}> k$:

				\begin{enumerate}
					\item Find pairwise nearest clusters $\mathcal{D}_i$, $\mathcal{D}_j$.
					\item Merge $\mathcal{D}_i$ and $\mathcal{D}_j$.
					\item Update $\hat{k} -= 1$.
				\end{enumerate}

		\end{enumerate}

		The stopping criterion can be a threshold on pairwise similarity.

	\subsection{Measuring cluster similarities}

	\begin{itemize}
		\item Nearest-neighbour:

			$$d_{\min}(\mathcal{D}_i,\mathcal{D}_j) = \min\limits_{\vec{x}\in\mathcal{D}_i,\vec{x}'\in\mathcal{D}_j} ||\vec{x}-\vec{x}'||$$

		\item Farthest-neighbour:

			$$d_{\max}(\mathcal{D}_i,\mathcal{D}_j) = \max\limits_{\vec{x}\in\mathcal{D}_i,\vec{x}'\in\mathcal{D}_j} ||\vec{x}-\vec{x}'||$$

		\item Average distance:

			$$d_{avg}(\mathcal{D}_i,\mathcal{D}_j) = \frac{1}{n_in_j}\sum\limits_{\vec{x}\in\mathcal{D}_i}\sum\limits_{\vec{x}'\in\mathcal{D}_j} ||\vec{x}-\vec{x}'||$$

		\item Distance between means:

			$$d_{means}(\mathcal{D}_i,\mathcal{D}_j) = ||\vec{\mu}_i-\vec{\mu}_j||$$

	\end{itemize}

	Nearest-neighbour and farthest-neighbour are more sensitive to outliers.



	\subsection{Stepwise optimal hierarchical clustering}

		\subsubsection{Algorithm}

		\begin{enumerate}
			\item Initialize: final cluster number $k$, initial cluster number $\hat{k} = n$ and initial clusters $\mathcal{D}_i = \{x_i\},i\in\{1,\dots,n\}$.
			\item While $\hat{k}> k$:

				\begin{enumerate}
					\item Find best clusters $\mathcal{D}_i$, $\mathcal{D}_j$ to merge according to the evaluation criterion.
					\item Merge $\mathcal{D}_i$ and $\mathcal{D}_j$.
					\item Update $\hat{k} -= 1$.
				\end{enumerate}

		\end{enumerate}
		

