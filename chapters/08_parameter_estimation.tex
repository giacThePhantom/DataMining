\chapter{Parameter estimation}

\section{Introduction}

	\subsection{Setting}
	Data is sampled from a probability distribution $p(x, y)$, whose form is known but its parameters are unknown.
	The training set $\mathcal{D} = \{(x_1, y_1), \dots, (x_m, y_m)\}$ of examples sampled independent and identically distributed according to $p(x,y)$.

	\subsection{Task}
	The task is to estimate the unknown parameters of $p$ from training data $\mathcal{D}$.
	This is the same as Bayesian decision theory: there is a need to compute the posterior probability of classes given examples, except the parameters of the distributions are unknown and a training set is provided instead.

	\subsection{Multi class classification}
	The training set can be divided into $\mathcal{D}_1, \dots, \mathcal{D}_c$ subsets, one for each class such that $\mathcal{D}_i = \{\vec{x}_1, \dots, \vec{x}_n\}$ contains independent and identically distributed examples for target class $y_i$.
	For any new example $\vec{x}$ the posterior probability of the class given the example and the full training set $\mathcal{D}$ is computed:

	$$P(y_i|\vec{x}, \mathcal{D}) = \frac{p(\vec{x}|y_i, \mathcal{D})p(y_i|\mathcal{D})}{p(\vec{x}|\mathcal{D}}$$

		\subsubsection{Simplifications}
		$\vec{x}$ can be assumed independent of $\mathcal{D}_j$ with $j\neq i$ given $y_i$ and $\mathcal{D}_i$.
		Without additional knowledge $p(y_i|\mathcal{D})$ can be computed as the fraction of examples with that class in the dataset.
		The normalizing factor $p(\vec{x}|\mathcal{D})$ can be computed marginalizing $p(\vec{x}|y_i, \mathcal{D}_i)p(y_i|\mathcal{D})$ over possible classes.
		There is a need to estimate class-dependent parameters $\vec{\theta}_i$ for $p(\vec{x}|y_i, \mathcal{D}_i$.

\section{Maximum likelihood}
Maximum likelihood or maximum a posteriori estimation assumes parameters $\vec{\theta}_i$ have fixed but unknown values.
These are computed as those maximizing the probability of the observed examples $\mathcal{D}_i$.
Obtained values are used to compute the probability for new examples:

$$p(\vec{x}|y_i, \mathcal{D}_i)\approx p(\vec{x}|\vec{\theta}_i)$$

It assumes a prior distribution for the parameters $p(\vec{\theta}_i)$ is available.
This maximizes the likelihood of the parameters with respect to the training samples and there is no assumption about prior distributions for parameters.

$$\vec{\theta}_i^* = \arg\max\limits_{\vec{\theta}_i} = p(\vec{\theta}_i|\mathcal{D}_i, y_i) = \arg\max\limits_{\vec{\theta}_i}p(\mathcal{D}_i, y_i|\vec{\theta}_i)p(\vec{\theta}_i) = \arg\max\limits_{\vec{\theta}_i}p(\mathcal{D}_i, y_i|\vec{\theta}_i)$$

	\subsection{Setting}
	A training data $\mathcal{D} = \{\vec{x}_1, \dots, \vec{x}_n\}$ of independent and identically distributed examples for the target class $y$ is available.
	Assuming the parameter vector $\vec{\theta}$ as a fixed but unknown value, the value maximizing its likelihood with respect to the training data is estimated:

	$$\vec{\theta}^* = \arg\max\limits_{\vec{\theta}}p(\mathcal{D}|\vec{\theta}) = \arg\max\limits_{\vec{\theta}}\prod\limits_{j=1}^n p(\vec{x}_j|\vec{\theta})$$

	The joint probability over $\mathcal{D}$ decomposes into a product as examples are independent and identically distributed.

	\subsection{Maximizing log-likelihood}
	It is usually simpler to maximize the logarithm of the likelihood because of its monotonic nature:

	$$\vec{\theta}^* = \arg\max\limits_{\vec{\theta}}\ln p(\mathcal{D}|\vec{\theta}) = \arg\max\limits_{\vec{\theta}}\sum\limits_{j=1}^n\ln p(\vec{x}_j|\vec{\theta})$$

	The necessary conditions for the maximum can be obtained zeroing the gradient with respect to $\vec{\theta}$:

	$$\nabla_{\vec{\theta}}\sum\limits_{j=1}^n\ln p(\vec{x}_j|\vec{\theta}) = \vec{0}$$

	Points zeroing the gradient can be local or global maxima depending on the form of the distribution.

	\subsection{Univariate Gaussian case}
	For the univariate Gaussian with unknown $\mu$ and $\sigma^2$ the log likelihood is:

	$$\mathcal{L} = \sum\limits_{j=1}^n-\frac{1}{\sigma^2}(x_j-\mu)^2-\frac{1}{2}\ln 2\pi\sigma^2$$

		\subsubsection{Mean}
		The gradient with respect to $\mu$ is:

		$$\frac{\partial\mathcal{L}}{\partial\mu} = 2\sum\limits_{j=1}^n-\frac{1}{2\sigma^2}(x_j - \mu)(-1) = \sum\limits_{j=1}^n\frac{1}{\sigma^2}(x_j-\mu)$$

		Setting the gradient to zero gives mean:

		\begin{align*}
			\sum\limits_{j=1}^n\frac{1}{\sigma^2}(x_j-\mu) &= 0 = \sum\limits_{j=1}^n(x_j-\mu)\\
			\sum\limits_{j=1}^n x_j &= \sum\limits_{j=1}^n\mu\\
			\sum\limits_{j=1}^n & = n\mu\\
			\mu &= \frac{1}{n}\sum\limits_{j=1}^nx_j
		\end{align*}

		\subsubsection{Variance}
		The gradient with respect to $\sigma^2$ is:

		\begin{align*}
			\frac{\partial\mathcal{L}}{\partial\sigma^2} &= \sum\limits_{j=1}^n-(x_j-\mu)^2\frac{\partial}{\partial\sigma^2}\frac{1}{2\sigma^2}-\frac{1}{2}\frac{1}{2\pi\sigma^2}2\pi = \\
																									 &=\sum\limits_{j-1}^n - (x_j - \mu)^2\frac{1}{2}(-1)\frac{1}{\sigma^4}-\frac{1}{2\sigma^2}
		\end{align*}

		Setting the gradient to zero gives variance:

		\begin{align*}
			\sum\limits_{j=1}^n\frac{1}{2\sigma^2} = \sum\limits_{j=1}^n\frac{(x_j-\mu)^2}{2\sigma^4}\\
			\sum\limits_{j=1}^n\sigma^2 &= \sum\limits_{j=1}^n(x_j-\mu)^2\\
			\sigma^2 = \frac{1}{n}\sum\limits_{j=1}^n(x_j-\mu)^2
		\end{align*}

	\subsection{Multivariate Gaussian case}
	For the multivariate Gaussian with unknown $\vec{\mu}$ and $\Sigma$ the log-likelihood is:

	$$\sum\limits_{j=1}^n-\frac{1}{2}(\vec{x}_j -\vec{\mu})^t\Sigma^{-1}(\vec{x}_j - \vec{\mu}) - \frac{1}{2}\ln(2\pi)^d|\Sigma|$$

	The maximum-likelihood estimates are:

	$$\vec{\mu} = \frac{1}{n}\sum\limits_{j=1}^n\vec{x}_j$$

	$$\Sigma = \frac{1}{n}\sum\limits_{j=1}^n(\vec{x}_j-\vec{\mu})(\vec{x}_j-\vec{\mu})^t$$

		\subsubsection{Proof for the mean}
		The gradient with respect to the mean is:

		\begin{align*}
			\nabla\vec{\mu}\sum\limits_{j=1}^n-\frac{1}{2}(\vec{x}_j-\vec{\mu})^t\Sigma^{-1}(\vec{x}_j-\vec{\mu})-\frac{1}{2}\ln(2\pi)^d|\Sigma|=\\
			\sum\limits_{j=1}^n\Sigma^{-1}(\vec{x}_j-\vec{\mu})
		\end{align*}

		Noting that $\frac{\partial}{\partial\vec{x}}\vec{x}^TA\vec{x} = A^T\vec{x}+A\vec{x} = 2A\vec{x}$ for symmetric $A$.
		Setting the gradient to zero:

		\begin{align*}
			\sum\limits_{j=1}^n\Sigma^{-1}(\vec{x}_j-\vec{\mu}) &=\vec{0}\\
			\sum\limits_{j=1}^n(\vec{x}_j-\vec{\mu}) &=\Sigma\vec{0} = \vec{0}\\
			\sum\limits_{j=1}^n\vec{x}_j &=\sum\limits_{j=1}^n\vec{\mu} = n\vec{\mu}\\
			\vec{\mu} &=\frac{1}{n}\sum\limits_{j=1}^n\vec{x}_j
		\end{align*}

		\subsubsection{Proof for the covariance}
		The gradient with respect to the covariance is:

		\begin{align*}
			\frac{\partial}{\partial\Sigma} \sum\limits_{j=1}^n-\frac{1}{2}(\vec{x}_j-\vec{\mu})^t\Sigma^{-1}(\vec{x}_j-\vec{\mu})-\frac{1}{2}\ln(2\pi)^d|\Sigma|=\\
			-\frac{1}{2}\biggl(\sum\limits_{j=1}^n\frac{\partial}{\partial\Sigma}(\vec{x}_j-\vec{\mu})^t\Sigma^{-1}(\vec{x}_j-\vec{\mu})+\sum\limits_{j=1}^n\frac{\partial}{\partial\Sigma}\ln(2\pi)^d|\Sigma|\biggr)
		\end{align*}

		Now, expanding the first derivative:

		\begin{align*}
			\frac{\partial}{\partial\Sigma}(\vec{x}_j-\vec{\mu})^t\Sigma^{-1}(\vec{x}_j-\vec{\mu}) =\\
			(\vec{x}_j-\vec{\mu})(\vec{x}_j-\vec{\mu})^t\frac{\partial}{\partial\Sigma}\Sigma^{-1} =\\
			-(\vec{x}_j-\vec{\mu})(\vec{x}_j-\vec{\mu})^t\Sigma^{-2}
		\end{align*}

		Using the matrix derivative rule: $\frac{\partial}{\partial B} tr(ABC) = CA$, where $A=(\vec{x}_j-\vec{\mu})^t$, $B = \Sigma^{-1}$ and $C=(\vec{x}_j-\vec{\mu})$ and $tr(ABC) = ABC$ as $ABC$ is scalar.
                Now, expanding the second derivative:

                \begin{align*}
                  \frac{\partial}{\partial\Sigma}\ln(2\pi)^d|\Sigma| &=\frac{1}{(2\pi)^d}|\Sigma|^{-1}\frac{\partial}{\partial\Sigma}(2\pi)^d|\Sigma|=\\
                    \frac{1}{(2\pi)^d}|\Sigma|^{-1}(2\pi)^d\frac{\partial}{\partial\Sigma}|\Sigma| &=|\Sigma|^{-1}|\Sigma|\Sigma^{-1} = \Sigma^{-1}
                \end{align*}

                Using the matrix derivative rule: $\frac{\partial}{\partial A} |A| = |A|A^{-1}$.
                Combining those results and putting them equal to zero:

                $$-\frac{1}{2}\biggl(\sum\limits_{j=1}^n\overbrace{-(\vec{x}_j-\vec{\mu})(\vec{x}_j-\vec{\mu})^t\Sigma^{-2}}^{\frac{\partial}{\partial\Sigma}(\vec{x}_j-\vec{\mu})^t\Sigma^{-1}(\vec{x}_j-\vec{\mu})}+\sum\limits_{j=1}^n\overbrace{\Sigma^{-1}}^{\frac{\partial}{\partial\Sigma}\ln(2\pi)^d|\Sigma|}\biggr)=0$$

								\begin{align*}
									\sum\limits_{j=1}^n\Sigma^{-1} &= \sum\limits_{j=1}^n(\vec{x}_j-\vec{\mu})(\vec{x}_j-\vec{\mu})^t\Sigma^{-2}\\
									\Sigma^2\sum\limits_{j=1}^n\Sigma^{-1}&=\Sigma^2\sum\limits_{j=1}^n(\vec{x}_j-\vec{\mu})(\vec{x}_j-\vec{\mu})^t\Sigma^{-2}\\
									\sum\limits_{j=1}^n\Sigma &= \sum\limits_{j=1}^n(\vec{x}_j-\vec{\mu})(\vec{x}_j-\vec{\mu})^t\\
									n\Sigma &= \sum\limits_{j=1}^n(\vec{x}_j-\vec{\mu})(\vec{x}_j-\vec{\mu})^t\\
									\Sigma &= \frac{1}{n}\sum\limits_{j=1}^n(\vec{x}_j-\vec{\mu})(\vec{x}_j-\vec{\mu})^t
								\end{align*}



	\subsection{General Gaussian case}
	In the case of a general Gaussian the maximum likelihood estimates for Gaussian parameters are their empirical estimates over the samples: the mean is the sample mean and the covariance matrix is the mean of the sample covariances.


\section{Bayesian estimation}
Bayesian estimation assumes parameters $\vec{\theta}_i$ are random variables with some known prior distribution.
Observing examples turns prior distribution over parameters into a posterior distribution.
Predictions for new examples are obtained integrating over all possible values for the parameters:

$$p(\vec{x}|y_i, \mathcal{D}_i) = \int_{\vec{\theta}_i}p(\vec{x}, \vec{\theta}_i|y_i, \mathcal{D}_i)d\vec{\theta}_i$$

	\subsection{Setting}
	Bayesian estimation assumes parameters $\vec{\theta}_i$ are random variables with some known prior distribution.
	Predictions for new examples are obtained integrating over all possible values for the parameters:

	$$p(\vec{x}|y_i, \mathcal{D}_i) = \int_{\vec{\theta}_i}p(\vec{x}, \vec{\theta}_i|y_i, \mathcal{D}_i)d\vec{\theta}_i$$

	Because probability of $\vec{x}$ given each class $y_i$ is independent of the other classes $y_i$:

	$$p(\vec{x}|\mathcal{D}) = \int_{\vec{\theta}} p(\vec{x}, \vec{\theta}|mathcal{D})d\vec{\theta} =\int p(\vec{x}|\vec{\theta})p(\vec{\theta}|\mathcal{D})d\vec{\theta}$$

	$p(\vec{x}|\vec{\theta})$ can be easily computed because the form and the parameters of the distribution are known, so there is a need to estimate the parameter posterior density given the training set:

	$$p(\vec{\theta}|\mathcal{D}) = \frac{p(\mathcal{D}|\vec{\theta})p(\vec{\theta})}{p(\mathcal{D})}$$

	$P(\mathcal{D})$ is a constant independent of $\vec{\theta}$ so it will not influence the final Bayesian decision, if the final probability is needed it can be computed:

	$$P(\mathcal{D}) = \int_{\vec{\theta}}p(\mathcal{D}|\vec{\theta})p(\vec{\theta})d\vec{\theta}$$

	\subsection{Univariate normal case - unknown $\mu$, known $\sigma^2$}
	In this case the examples are drawn from $p(x|\mu)\sim N(\mu, \sigma^2)$.
	The Gaussian mean prior distribution is itself normal: $p(\mu) \sim N(\mu_0, \sigma^2_0)$.
	The Gaussian mean posterior given the dataset is computed as:

	$$p(\mu|\mathcal{D}) = \frac{p(\mathcal{D}|\mu)p(\mu)}{p(\mathcal{D}} = \alpha\prod\limits_{j=1}^np(x_j|\mu)p(\mu)$$

	Where $\alpha = \frac{1}{p(\mathcal{D})}$ is independent of $\mu$.

		\subsubsection{A posteriori parameter density}

		\begin{align*}
			p(\mu|\mathcal{D}) &= \alpha\prod\limits_{j = 1}^n\overbrace{\frac{1}{\sqrt{2\pi}\sigma}e^{[-\frac{1}{2}(\frac{x_j-\mu}{\sigma})^2]}}^{p(x_j|\mu)}\overbrace{\frac{1}{\sqrt{2\pi}\sigma_0}e^{[-\frac{1}{2}(\frac{\mu-\mu_0}{\sigma_0})^2]}}^{p(\mu)} =\\
												&=\alpha'e^{[-\frac{1}{2}(\sum\limits_{j=1}^n(\frac{\mu-x_j}{\sigma})^2+(\frac{\mu-\mu_0}{\sigma_0})^2)]} =\\
												&=\alpha''e^{[-\frac{1}{2}[(\frac{n}{\sigma^2}+\frac{1}{\sigma^2_0})\mu^2 - 2(\frac{1}{\sigma^2}\sum\limits_{j=1}^nx_j + \frac{\mu_0}{\sigma^2_0})\mu]]}
		\end{align*}

		Where the normal distribution:

		$$p(\mu|\mathcal{D}) = \frac{1}{\sqrt{2\pi}\sigma}e^{[-\frac{1}{2}(\frac{\mu-\mu_n}{\sigma^n})^2]}$$

		\subsubsection{Recovering mean and variance}

		\begin{align*}
			\biggl(\frac{n}{\sigma^2} + \frac{1}{\sigma^2_0}\biggr)\mu^2 -2\biggl(\frac{1}{\sigma^2}\sum\limits_{j=1}^nx_j + \frac{\mu_0}{\sigma_0^2}\biggr)\mu + \alpha''' &=\biggl(\frac{\mu-\mu_n}{\sigma_n}\biggr)^2\\
																																																																																			&=\frac{1}{\sigma^2_n}\mu^2 - 2\frac{\mu_n}{\sigma^2_n}\mu + \frac{\mu_n^2}{\sigma^2_n}
		\end{align*}

		Solving for $\mu_n$ and $\sigma^2_n$:

		$$\mu_n = \biggl(\frac{n\sigma^2_0}{n\sigma^2_o+\sigma^2}\biggr)\bar{\mu}_n + \frac{\sigma^2}{n\sigma^2_0 + \sigma^2}\mu_0\qquad \sigma^2_n = \frac{\sigma^2_0\sigma^2}{n\sigma^2_0+\sigma^2}$$

		Where the sample mean $\bar{\mu}_n = \frac{1}{n}\sum\limits_{j=1}^n x_j$.

		\subsubsection{Interpreting the posterior}
		The mean is a linear combination of the prior $\mu_0$ and the sample means $\bar{\mu}_n$.
		The more training examples the more sample mean dominates over the prior mean.
		The more training examples, the more variance decreases making the distribution sharply peaked over its mean:

		$$\lim\limits_{n\rightarrow\infty}\frac{\sigma_0^2\sigma^2}{n\sigma^2_0+\sigma^2} = \lim\limits_{n\rightarrow\infty}\frac{\sigma^2}{n} = 0$$

		\subsubsection{Computing the class conditional density}

		\begin{align*}
			p(x|\mathcal{D}) &= \int p(x|\mu)p(\mu|\mathcal{D})d\mu=\\
											 &=\int\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}\frac{1}{\sqrt{2\pi}\sigma_n}e^{-\frac{1}{2}(\frac{\mu-\mu_n}{\sigma_n})^2}d\mu=\\
											 &\sim N(\mu_n, \sigma^2 + \sigma^2_n)
		\end{align*}

		The probability of $x$ given the dataset for the class is a Gaussian with mean equal to the posterior mean, variance equal to the sum of the known variance $\sigma^2$ and an additional variaci $\sigma^2_n$ due to the uncertainty of the mean.

	\subsection{Multivariate normal case - unknown $\mu$, known $\Sigma$}
	This is a generalization of the univariate case:

	\begin{multicols}{2}
		\begin{itemize}
			\item $p(\vec{x}|\vec{\mu}) \sim N(\vec{\mu}, \Sigma)$
			\item $p(\vec{\mu})\sim N(\vec{\mu}_0, \Sigma_0)$
			\item $p(\vec{\mu}|\mathcal{D}) \sim N(\vec{\mu}_n, \Sigma_n)$
			\item $p(\vec{x}|\mathcal{D}) \sim N(\vec{\mu}_n, \Sigma+\Sigma_n)$
		\end{itemize}
	\end{multicols}

	\subsection{Gamma distribution}
	The gamma distribution is defined in the interval $[0, \infty]$.
	Its parameters are $\alpha>0$ the shape and $\beta>0$ the rate.
	Its probability density function is:

	$$p(x;\alpha, \beta) = \frac{\mu^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$$

	With $\mathbb{E}[x] = \frac{\alpha}{\beta}$ and $Var[x] = \frac{\alpha}{\beta^2}$.
	It is used to model the prior distribution of the precision.

	\subsection{Univariate normal case - unknown $\mu$ and $\lambda = \frac{1}{\sigma^2}$}
	In this case the samples are drawn from $p(x|\mu, \lambda)\sim N(\mu, \frac{1}{\lambda}$.
	The prior of mean and precision is the Normal Gamma distribution:

	\begin{align*}
		p(\mu, \lambda) &=p(\mu|\lambda)p(\lambda) = N\biggl(\mu|\mu_0, \frac{1}{k_0 \lambda}\biggr)Ga(\lambda|\alpha_0, \beta_0)=\\
										&=NG(\mu, \lambda|\mu_0, k_0, \alpha_0, \beta_0)
	\end{align*}

		\subsubsection{A posteriori parameter density}

		\begin{align*}
			p(\mu, \lambda|\mathcal{D})& = \frac{1}{\mathcal{D}}\prod\limits_{j=1}^n\overbrace{\frac{\lambda^{\frac{1}{2}}}{\sqrt{2\pi}}e^{-\frac{\lambda}{2}(x_j-\mu)^2}}^{p(x_j|\mu, \lambda)}\overbrace{\frac{(k_0\lambda)^{\frac{1}{2}}}{\sqrt{2\pi}}e^{-\frac{k_0\lambda}{2}(\mu-\mu_0)^2}}^{p(\mu|\lambda)}\overbrace{\frac{\beta^{\alpha_0}_0}{\Gamma(\alpha_0)}\lambda^{\alpha_0-1}e^{-\beta_0\lambda}}^{p(\lambda)}\propto\\
									&\propto \lambda^{\alpha_0+\frac{n}{2}-1}e^{-\beta_0\lambda}\lambda^{\frac{1}{2}}e^{\frac{\lambda}{2}[\sum\limits_{j=1}^n(x_j-\mu)^2-k_0(\mu-\mu_0)^2}
		\end{align*}

		The posteriori parameter density is still a Normal Gamma distribution:

		$$p(\mu, \lambda|\mathcal{D}) = NG(\mu, \lambda|\mu_n, k-n, \alpha_n, \beta_n)$$
		Where:

		\begin{itemize}
			\item $\mu_n = \frac{k_0\mu_0+n\bar{\mu}_n}{k_0+n}$, the weighted average of prior $\mu_0$ and sample means $\mu_n$, weighted by $k_0$ and $n$ respectively.
			\item $k_n = k_0+n$, increased by the number of samples.
			\item $\alpha_n = \alpha_0 + \frac{n}{2}$ increased by half the number of samples.
			\item $\beta_n = \beta_0+\frac{1}{2}\sum\limits_{j=1}^n(x_j-\bar{\mu}_n)^2+\frac{k_0n(\bar{\mu}_n-\mu_0)^2}{2(k_0+n)}$, this is the sum of prior sum of sqares $\beta_0$ and sample um of squares and a term due to the discrepancy between the sample mean and the prior mean.
		\end{itemize}

		\subsubsection{Computing the posterior predictive}

		\begin{align*}
			p(x|\mathcal{D}) &= \int_\mu\int_\lambda p(x|\mu, \lambda)p(\mu, \lambda|\mathcal{D})d\mu d\lambda\\
											&=\frac{P(x, \mathcal{D})}{P(\mathcal{D})} = t_{2\alpha_n}\biggl(x|\mu_n, \frac{\beta_n(k_n+1)}{\alpha_n k_n}\biggr)
		\end{align*}

		This is a T-distribution with mean $\mu_n$ and precision $\frac{\beta_n(k_n+1)}{\alpha_n k_n}$.

	\subsection{Wishart distribution}
	The Wishart distribution is defined over $d\times d$ a positive semi-definite matrix.
	Its parameters are $\nu>d-1$, the degrees of freedon adn $T>0$ the $d\times d$ scale matrix.
	Its probability density function is:

	$$p(X;\nu, T) = \frac{1}{2^{\nu\frac{d}{2}}|T|^{\frac{\nu}{2}}\Gamma_d(\frac{\nu}{2})}|X|^{\frac{\nu-d-1}{2}}e^{-\frac{1}{2}tr(T^{-1}X)}$$

	With $\mathbb{E}[X] = \nu T$ and $Var[X_{ij}] = \nu(T_ii^2+T_{ii}T_{jj})$.
	It is used to model the prior distribution of the precision matrix.
	$T$ is the prior covariance.

	\subsection{Multivariate normal case - unknown $\mu$ and $\Sigma$}
	In this case the examples are drawn from $p(\vec{x}|\vec{\mu}, \Lambda)\sim N(\vec{\mu}, \Lambda^{-1})$.
	The prior of mean and precision is the Normal Wishart distribution:

	$$p(\vec{\mu}, \Lambda) = p(\vec{\mu}|\Lambda)p(\Lambda) = N(\vec{\mu}|\vec{\mu}_0, (k_0\Lambda)^{-1})Wi(\Lambda|\nu, T)$$

		\subsubsection{A posteriori parameter density}

		$$p(\vec{\mu}, \Lambda) = N(\vec{\mu}|\vec{\mu}_0, (k_0\Lambda)^{-1})Wi(\Lambda|\nu, T)$$

		Where:

		\begin{itemize}
			\item $\mu_n = \frac{k_0\mu_0+n\bar{\mu}_n}{k_0+n}$
			\item $T_n = T+\sum\limits_{i=1}^n(x_i-\bar{\mu}_n)(x_i-\bar{\mu}_n)^T + \frac{kn}{k+n}(\mu_0-\bar{\mu}_n)(\mu_0-\bar{\mu})n)^T$
			\item $\nu_n = \nu+n$
			\item $k_n = k+n$
		\end{itemize}

		\subsubsection{Computing the posterior predictive}

		$$p(x|\mathcal{D}) = t_{\nu-d+1}\biggl(x|\mu_n, \frac{T_n(k_n+1)}{k_n(\nu_n-d+1)}\biggr)$$

\section{Sufficient statistics}

	\subsection{Definition}
	Any function on a set of samples $\mathcal{D}$ is a statistic.
	A statistic $s = \phi(\mathcal{D})$ is sufficient for some parameters $\vec{\theta}$ if $P(\mathcal{D}|\vec{s}, \vec{\theta}) = P(\mathcal{D}|\vec{s})$.
	If $\vec{\theta}$ is a random variable, a sufficient statistic contains all relevant information $\mathcal{D}$ has for estimating it:

	$$p(\vec{\theta}|\mathcal{D}, \vec{s}) = \frac{p(\mathcal{D}|\vec{\theta}, \vec{s})p(\vec{\theta}|\vec{s})}{p(\mathcal{D}|\vec{s})} = p(\vec{\theta}|\vec{s})$$

	A sufficient statistic allows to compress a sample $\mathcal{D}$ into fewer values.
	Sample mean and covariance are sufficient statistics for true mean and covariance of the Gaussian distribution.

	\subsection{Conjugate priors}
	Given a likelihood function $p(\vec{x}|\vec{\theta})$ and a prior distribution $p(\vec{\theta})$, $p(\vec{\theta})$ is a conjugate prior for $p(\vec{x}|\vec{\theta})$ if the posterior distribution $p(\vec{\theta}|\vec{x})$ is in the same family as the prior $p(\vec{\theta})$

\section{Bernoulli distribution}

	\subsection{Setting}
	The Bernoulli distribution represent a boolean event with $x=1$ for success and $x=0$ for failure.
	Its parameter is $\theta$, the probability of success.
	Its probability mass function is:

	$$P(x|\theta) = \theta^x(1-\theta)^{1-x}$$

	Its beta conjugate prior is:

	$$P(\theta|\psi) = P(\theta|\alpha_h, \alpha_t) = \frac{\Gamma(\alpha)}{\Gamma(\alpha_h)\Gamma(\alpha_t)}\theta^{\alpha_h-1}(1-\theta)^{\alpha_t-1}$$

	\subsection{Maximum likelihood estimation}
	Let $\mathcal{D} = \{H, H, T, T, T, H, H\}$ of $N$ realizations.
	Its likelihood function is:

	$$p(\mathcal{D}|\theta) = \theta\cdot\theta\cdot(1-\theta)\cdot(1-\theta)\cdot(1-\theta)\cdot\theta\cdot\theta = \theta^h(1-\theta)^t$$

	Its maximum likelihood parameter:

	\begin{align*}
		\frac{\partial}{\partial\theta}\ln p(\mathcal{D}|\theta) = 0\Rightarrow \frac{\partial}{\partial\theta}h\ln\theta + t\ln(1-\theta) &=0\\
		h\frac{1}{\theta}-t\frac{1}{1-\theta}&=0\\
		h(1-\theta) &= t\theta\\
		\theta &= \frac{h}{h+t}
	\end{align*}

	$h$ and $t$ are the sufficient statistics.

	\subsection{Bayesian estimation}
	Parameter posterior is proportional to:

	$$P(\theta|\mathcal{D}, \psi)\propto P(\mathcal{D}|\theta)P(\theta|\psi)\propto \theta^t(1-\theta)^t\theta^{\alpha_h-1}(1-\theta)^{\alpha_t-1}$$

	The posterior has a beta distribution with parameters $h+\alpha_h$ and $t+\alpha_t$:

	$$P(\theta|\mathcal{D}, \psi)\propto\theta^{h+\alpha_h-1}(1-\theta)^{t+\alpha_t-1}$$

	The prediction for a new event is the expected value of the posterior beta:

	\begin{align*}
		P(x|\mathcal{D}) &=\int P(x|\theta)P(\theta|\mathcal{D}, \psi)d\theta & = \int \theta P(\theta|\mathcal{D}, \psi)d\theta\\
										 &=\mathbb{E}_{P(\theta|\mathcal{D}, \psi)}[\theta] &= \frac{h+\alpha_h}{h+t+\alpha_h+\alpha_t}
	\end{align*}

		\subsubsection{Interpreting priors}
		The prior knowledge is encoded as the number $\alpha=\alpha_h+\alpha_t$ of imaginary experiments.
		It is assumed $\alpha_t$ times heads was observed.
		$\alpha$ is the equivalent sample size and $\alpha\rightarrow 0$ reduces the estimation to the classical maximum likelihood approach.

\section{Multinomial distribution}

	\subsection{Setting}
	The multinomial distribution models categorical event with $r$ states $x\in\{x^1,\dots,x^r\}$.
	One-hot encoding $\vec{z}(x) = [z_1(x), \dots, z_r(x)]$ with $z_k(x) = 1$ if $x=x^k$, $o$ otherwise.
	Its parameters are $\vec{\theta} = [\theta_1, \dots, \theta_r]$ the probability  of each state.
	Its probability mass function is:

	$$P(x|\vec{\theta}) = \prod\limits_{k=1}^r\theta_k^{z_k(x)}$$

	Its Dirichlet conjugate prior:

	$$P(\vec{\theta}|\psi) = P(\vec{\theta}|\alpha_1, \dots, \alpha_r) = \frac{\Gamma(\alpha)}{\prod\limits_{k=1}^r\Gamma(\alpha_k)}\prod\limits_{k=1}^r\theta_k^{\alpha_k-1}$$

	\subsection{Maximum likelihood estimation}
	Let $\mathcal{D}$ a dataset of $N$ realizations.
	The likelihood function is:

	$$p(\mathcal{D}|\vec{\theta} = \prod\limits_{j=1}^N\prod\limits_{k=1}^r\theta_k^{z_k(x_j)} = \prod\limits_{k=1}^r\theta_k^{N_k}$$

	The maximum likelihood parameter is $\theta_k= \frac{M_k}{N}$ and $N_1, \dots, N_r$ are the sufficient statistics.

	\subsection{Bayesian estimation}
	The parameter posterior is proportional to:

	$$P(\vec{\theta}|\mathcal{D}, \psi)\propto P(\mathcal{D}|\vec{\theta})P(\theta|\psi)\propto\prod\limits_{k=1}^r\theta_k^{N_k}\theta_k^{\alpha_k-1}$$

	The posterior has a Dirichlet distribution with parameters $N_k+\alpha_k$, where $k = 1, \dots, r$"

	$$P(\vec{\theta}|\mathcal{D}, \psi)\propto \prod\limits_{k=1}^r\theta_k^{N_k+\alpha_k-1}$$

	The prediction for a new event is the expected value of the posterior Dirichlet:

	$$P(x_k|\mathcal{D}) = \int\theta_k P(\vec{\theta}|\mathcal{D}, \psi)d\vec{\theta} = \mathbb{E}_{P(\vec{\theta}|\mathcal{D}, \psi)}[\theta_k] = \frac{N_k+\alpha_k}{N+\alpha}$$
