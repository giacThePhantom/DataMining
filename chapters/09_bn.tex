\chapter{Bayesian networks}

\section{Introduction}
All probabilistic inference and learning amount at repeated applications of the sum and product rules.
Probabilistic graphical models are graphical representation of the qualitative aspects of probability distributions allowing to:

\begin{multicols}{2}
	\begin{itemize}
		\item Visualize the structure of a probabilistic model in a simple and intuitive way.
		\item Discover properties of the model such as conditional independences by inspecting the graph.
		\item Express complex computations for inference and learning in terms of graphical manipulation.
		\item Represent multiple probability distributions with the same graph abstracting from their quantitative aspects.
	\end{itemize}
\end{multicols}

	\subsection{Bayesian networks semantics}
	A Bayesian network or $BN$ structure $\mathcal{G}$ is a directed graphical model.
	Each node represents a random variable $x_i$.
	Each edge represents a direct dependency between two variables.
	The structure encodes these independence assumptions:

	$$\mathcal{I}_l(\mathcal{G}) = \{\forall u x_i \perp NonDescendants_{x_i}|Parents_{x_i}\} \qquad\perp \equiv Independent$$

	Each variable is independent of its non-descendants given its parents.
	These are the local independences encoded by the networks.

	\subsection{Graph and distributions}
	Let $p$ be a joint distribution over variables $\mathcal{X}$.
	Let $\mathcal{I}(p)$ be the set of independence assertions holding in $p$.
	$\mathcal{G}$ in as independence map I-map for $p$ if $p$ satisfies the local independences in $\mathcal{G}$.

	$$\mathcal{I}_l(\mathcal{G})\subseteq\mathcal{I}(p)$$

	The reverse is not necessarily true: there can be independences in $p$ that are not modelled by $\mathcal{G}$.

	\subsection{Factorization}
	$p$ factorizes according to $\mathcal{G}$ if:

	$$p(x_1, \dots, x_m) = \prod\limits_{i=1}^m p(x_i|Pa_{x_i})$$

	If $\mathcal{G}$ is an $I$-map for $p$ then $p$ factorizes according to $\mathcal{G}$.
	If $p$ factorizes according to $\mathcal{G}$ then $\mathcal{G}$ is an $I$-map for $p$.

		\subsubsection{Proof that from I-map follows factorization}
		If $\mathcal{G}$ is an $I$-map for $p$ then $p$ satisfies at least these local independences:

		$$\{\forall i x_i \perp NonDescendants_{x_i} | Parents_{x_i}\}$$

		Order variables in a topological order relative to $\mathcal{G}$:

		$$x_i\rightarrow x_j\Rightarrow i<j$$

		Decompose the joint probability using the chain rule as:

		$$p(x_1, \dots, x_m) = \prod\limits_{i = 1}^m p(x_i|x_1, \dots, x_{i-1})$$

		Local independences imply that for each $x_i$:

		$$p(x_i|x_1, \dots, x_{i-1}) = p(x_i | Pa_{x_i})$$

		\subsubsection{Proof that from factorization follows I-map}
		If $p$ factorizes according to $\mathcal{G}$ the joint probability can be written as:

		$$p(x_1, \dots, x_m) = \prod\limits_{i = 1}^m p(x_i|Pa_{x_i})$$

		Consider the last variable $x_m$.
		By the product and sum rules:

		$$p(x_m | x_1, \dots, x_{m-1}) = \frac{p(x_1, \dots, x_m)}{p(x_1, \dots, x_{m-1})} = \frac{p(x_1, \dots, x_m)}{\sum\limits_{x_m}p(x_1, \dots, x_m)}$$

		Applying factorization and isolating the only term containing $x_m$ we get:

		\begin{align*}
			=\frac{\prod\limits_{i=1}^mp(x_i|Pa_{x_i}}{\sum\limits_{x_m}\prod\limits_{i=1}^mp(x_i | Pa_{x_i})} =\\
			=\frac{p(x_m|Pa_{x_m})\prod\limits_{i=1}^{m-1}p(x_i | Pa_{x_i})}{\prod\limits_{i=1}^{m-1}p(x_i|Pa_{x_i})\sum\limits_{x_m}p(x_m|Pa_{x_m})}=\\
			=\frac{p(x_m|Pa_{x_m})}{\sum\limits_{x_m}p(x_m|Pa_{x_m})} = p(x_m|Pa_{x_m})
		\end{align*}

		This is extendable for all the variables going backward.

	\subsection{Bayesian network definition}
	A Bayesian network is a pair $(\mathcal{G}, p)$ where $p$ factorizes over $\mathcal{G}$ and it is represented as a set of conditional probability distributions associated with the nodes of $\mathcal{G}$.

		\subsubsection{Factorized probability}

		$$p(x_1, \dots, x_m) = \prod\limits_{i =1}^m p(x_i | Pa_{x_i})$$


\section{Conditional independence}
Two variables $a,b$ are independent if:

$$p(a,b) = p(a)p(b)$$

Two variables are conditionally independent given $c$ if:

$$p(a,b|c) = p(a|c)p(b|c)$$

Independence assumptions can be verified by repeated applications of sum and product rules.
Graphical models allow to directly verify them through the $d$-separation criterion.

	\subsection{D separation}

		\subsubsection{Tail to tail}
		Joint distribution:

		$$p(a,b,c) = p(a|c)p(b|c)p(c)$$

		$a$ and $b$ are not independent:

		$$p(a,b) = \sum\limits_cp(a|c)p(b|c)p(c) \neq p(a)p(b)$$

		$a$ and $b$ are conditionally independent given $c$:

		$$p(a,b|c) = \frac{p(a,b,c)}{p(c)} = p(a|c)p(b|c)$$

		$c$ is tail-to-tail with respect to the path $a\rightarrow b$ as it is connected to the tails of the two arrows.

		\subsubsection{Head to tail}
		Joint distribution:

		$$p(a,b,c) = p(b|c)p(c|a)p(a) = p(b|c)p(a|c)p(c)$$

		$a$ and $b$ are not independent:

		$$p(a,b) = p(a)\sum\limits_cp(b|c)p(c|a)\neq p(a)p(b)$$

		$a$ and $b$ are conditionally independent given $c$:

		$$p(a,b|c) = \frac{p(b|c)p(a|c)p(c)}{p(c)} = p(b|c)p(a|c)$$

		$c$ is head-to-tail with respect to the path $a\rightarrow b$ as it is connected to the head of an arrow and to the tail of the other.

		\subsubsection{Head to head}
		Joint distribution:

		$$p(a,b,c) = p(c|a,b)p(a)p(b)$$

		$a$ and $b$ are independent:

		$$p(a,b) = p(a)\sum\limits_cp(c|a,b)p(a)p(b)=p(a)p(b)$$

		$a$ and $b$ are not conditionally independent given $c$:

		$$p(a,b|c) = \frac{p(c|a,b)p(a)p(b)}{p(c)} \neq p(a|c)p(b|c)$$

		$c$ is head-to-head with respect to the path $a\rightarrow b$ as it is connected to the heads of the two arrows.

		\subsubsection{General head-to-head}
		Let a descendant of a node $x$ be any node which can be reached from $x$ with a path following the direction of the arrows.
		A head-to-head node $c$ unblocks the dependency path between its parents if either itself or any of its descendants receives evidence.

	\subsection{General d-separation criterion}
	The sets $A$ and $B$ are independent given $C$ ($A\perp B|C $) if they are d-separated by $C$.

		\subsubsection{d-separation definition}
		Given a generic Bayesian network and $A,B,C$ arbitrary non intersecting sets of node.
		The sets $A$ and $B$ are d-separated by $C$ ($dsep(A;B|C)$) if all paths from any node in $A$ to any node in $B$ are blocked.
		A path is blocked if it includes at least one node such that either:

		\begin{multicols}{2}
			\begin{itemize}
				\item The arrows on the path meet tail-to-tail or head-to-tail at a node in $C$.
				\item The arrows on the path meet head-to-head at a node and neither it nor any of its descendants is in $C$.
			\end{itemize}
		\end{multicols}

\section{BN independence revisited}

	\subsection{Independence assumptions}
	A BN stricture $\mathcal{G}$ encodes a set of local independence assumptions:

	$$\mathcal{I}_l(\mathcal{G}) = \{\forall i x_i \perp NonDescendants_{x_i} | Parents_{x_i}\}$$

	A BN structure $\mathcal{G}$ encodes a set of global or Markov independence assumptions:

	$$\mathcal{I}(\mathcal{G}) = \{(A\perp B |C):dsep(A;B|C)\}$$

	\subsection{BN equivalence classes}

		\subsubsection{I-equivalence}
		Quite different BN structures can encode the exact same set of independence assumptions.
		Two BN structures $\mathcal{G}$ and $\mathcal{G}'$ are I-equivalent if $\mathcal{I}(\mathcal{G})=\mathcal{I}(\mathcal{G}')$.
		The space of BN structures over $\mathcal{X}$ is partitioned into a set of mutually exclusive and exhaustive I-equivalence classes.

			\paragraph{Sufficient conditions}
			If two structures $\mathcal{G}$ and $\mathcal{G}'$ have the same skeleton and the same set of $v$-structures then they are I-equivalent.

			\paragraph{Necessary and sufficient conditions}
			Two structures $\mathcal{G}$ and $\mathcal{G}'$ are I-equivalent if and only if they have the same skeleton and the same set of immoralities.

			\paragraph{Equivalence class}

				\subparagraph{Partially directed acyclic graph}
				A partially directed acyclic graph or PDAG is an acyclic graph with both directed and undirected edges.

				\subparagraph{Representing an equivalence class}
				An equivalence class for a structure $\mathcal{G}$ can be represented by a PDAG $\mathcal{K}$ such that:

				\begin{multicols}{2}
					\begin{itemize}
						\item If $x\rightarrow y\in\mathcal{K}$ then $x\rightarrow y$ should appear in all structures which are I-equivalent to $\mathcal{G}$.
						\item If $x - y\in\mathcal{K}$ then we can find a structure $\mathcal{G}'$ that is I-equivalent to $\mathcal{G}$ such that $x\rightarrow y\in \mathcal{G}'$
					\end{itemize}
				\end{multicols}

				\subparagraph{Generating members}
				Representative from $\mathcal{K}$ can be obtained by adding directions to undirected edges.
				One needs to check that the resulting structure has the same set of immoralities as $\mathcal{K}$.

	\subsection{I-maps and distributions}

		\subsubsection{Minimal I-maps}
		For a structure $\mathcal{G}$ to be an $I$-map for $p$, it does not need to encode all its independences: a fully connected graph is an I-map of any $p$ defined over its variables.
		A minimal I-map for $p$ is an I-map $\mathcal{G}$ which can't be reduced into a $\mathcal{G}'\subset\mathcal{G}$ by removing edges that is also an I-map for $p$.
		A minimal I-map for $p$ does not necessarily capture all the independences in $p$.

		\subsubsection{Perfect maps P-maps}
		A structure $\mathcal{G}$ is a perfect map for $p$ if it captures all and only its dependences:

		$$\mathcal{I}(\mathcal{G}) = \mathcal{I}(p)$$

		The algorithm for finding such P-map is exponential in the in-degree of the P-map.
		The algorithm returns an equivalence class rather than a single structures.
		Not all distributions have a P-map: some cannot be modelled exactly by the BN formalism.

	\subsection{Building Bayesian networks}
	\begin{multicols}{2}
		\begin{enumerate}
			\item Get together with a domain expert.
			\item Define variables for entities that can be observed or that you can be interested in predicting.
			\item Try following causality considerations in adding edges.
			\item In defining probabilities for configurations never assign zero probabilities.
			\item If data is available, use them to help in learning parameters and structure.
		\end{enumerate}
	\end{multicols}

\section{Markov blanket or boundary}

	\subsection{Definition}
	Given a directed graph with $m$ nodes the Markov blanket of node $x_i$ is the minimal set of nodes making it $x_i$ independent on the rest of the graph:

	\begin{align*}
		p(x_i|x_{j\neq i}) = \frac{p(x_1, \dots, x_m)}{p(x_{j\neq i})} &= \frac{p(x_1, \dots, x_m)}{\int p(x_1, \dots, x_m)dx_i}=\\
																																	&=\frac{\prod\limits_{k=1}^mp(x_k|pa_k)}{\int\prod\limits_{k=1}^mp(x_k|pa_k)dx_i}
	\end{align*}

	All components which do not include $x_i$ will cancel between numerator and denominator.
	The only remaining components are:

	\begin{multicols}{2}
		\begin{itemize}
			\item $p(x_i|pa_i)$ the probability of $x_i$ given its parents.
			\item $p(x_j|pa_j)$ where $pa_j$ includes $x_i$, the children of $x_i$ with their co-parents.
		\end{itemize}
	\end{multicols}

	\subsection{d-separation}
	Each parent $x_j$ of $x_i$ will be head-to-tail or tail-to-tail in the path to $x_i$ and any of $x_j$ other neighbours, so the paths are blocked.
	Each child $x_j$ of $x_i$ will be head-to-tail in the path to $x_i$ and any of $x_j$ children, so the paths are blocked.
	Each co-parent $x_k$ of a child $x_j$ of $x_i$ will be head-to-tail or tail-to-tail in the path to $x_j$ and any of $x_k$ other neighbours, so all paths are blocked.
